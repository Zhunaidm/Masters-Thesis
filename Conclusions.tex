\chapter{Conclusion}
The aim for this thesis was to provide explanations into the decisions made by Neural Networks so that they may be considered as possible solutions to problems where interpretability is a strict requirement. We discuss the findings which we have gathered during this experiment and note some work that still has to be done.
\section{Conclusions}
\subsection{Explaining simple Neural Networks}
We were able to produce explanations for various different model architectures and have also proved that the explanations generated by LIME and SHAP are nearly identical to the weight coefficients extracted from linear models. We were able to deduce how much each feature contributes relative to their magnitude and to one another. Therefore we can conclude that these tools do provide interpretations into black-box networks, however we are only able to gather a simple relationship between the input and output layers, with \emph{Lucid} being the only tool which attempts to explain the relationship between hidden layers.
\subsection{Lucid is complicated to use}
From our evaluation of \emph{Lucid} we have shown that it requires considerable effort and probing into the network in order to gather any form of visualization. It is still an actively developed project and therefore there is not much documentation on how to adapt it for use with different model architectures. Although Lucid is hard to use for models which have already being built, it can be valuable while developing an image-based model to understand exactly what is happening within each layer.
\subsection{Exposing problems within the model}
As we have seen in Section \ref{sect-problem-expose} by using SHAP we were able to expose a misbehaving feature in our Neural Network. Although by simply viewing performance metrics, a model may seem to be performing well, by looking at the produced explanations we were able to notice that there was an issue either within our models architecture or the data itself. This is useful when testing whether a model is trustworthy enough to be used in production.
\subsection{Comparing network architectures}
As seen in Section \ref{sect-architecure} we were able to demonstrate how a change in a Neural Networks architecture affected the features which contribute towards the prediction. It is common when training a Neural Network to experiment with different hidden layers and activation functions. By making use of SHAP explanations this can help us identify which variations are best suited into solving our problem.
\subsection{Using SHAP for feature selection}
Looking at Section \ref{sect-prediction-strength} we have seen that by observing our SHAP explanations we were able to discover features which contribute little towards the prediction. Discovering variables with weak predictive strength was as simple as looking at how the SHAP value changed based on the features magnitude. This may be considered a manual process however it can scale well into many features, as it is ordered according to predictive strength. This can also be used when heavily correlated variables are discovered and a decision has to be made as to which to keep and which to remove.
\subsection{SHAP is in most ways superior to LIME}
Comparing the explanations of LIME to SHAP we can see that SHAP is more descriptive and provides more ways to express the explanations. The largest limitation of LIME is that it is only able to provide explanations for a single prediction at a time and the explanations provided can be seen as a local explanation. SHAP allows us to use a set of background samples where the SHAP values are calculated using all of these samples. SHAP also allows us to view explanations of multiple instances at a time in order to compare their magnitudes. This is to be expected since SHAP further expands upon the ideas introduced by LIME, therefore we are able to conclude that SHAP is the superior model agnostic explainer.
\section{Future Work}
\subsection{Complex networks}
All our experiments were based on simple architectures and therefore we do not know how well these tools scale for larger and more complex networks. Further experimentation has to be done in order to conclude that these tools are stable enough to be used for production models.
\subsection{Streamlining Lucid}
Lucid is the only tool which gains insight into the relationship of hidden layers. At the time of writing Lucid only works with Tensorflow 1 and is an extensive toolkit without much documentation on how to adapt it for use in custom models. It would be interesting to contribute to it's active research by providing functionality for platform independent Neural Networks and streamlining the process of interpretation.
\subsection{Explaining hidden layers}
Both LIME and SHAP are unable to provide insight into hidden layers which are arguably the most interesting part of Neural Networks. Further work could include adapting the concepts introduced in SHAP to be able to provide explanations between any two layers within a network.
\subsection{Fooling LIME and SHAP}
The paper \emph{Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods} \cite{slack2020fooling} provides a reason as to why the explanations of \emph{LIME} and \emph{SHAP} are not always to be trusted since they are prone to attacks by adversaries which aim to alter the produced explanations. A novel \emph{scaffolding} technique is introduced which effectively hides the bias of any given classifier. Using a real world dataset known as \emph{COMPAS} which has a inherent racist bias, the authors of the paper were able to fool LIME and SHAP into producing innocuous explanations that do not expose this bias. This is an exploit that has to be further explored and fixed before we can consider using them for models in production where they are open to attacks by adversaries.